#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
VLAè¯­è¨€æœåŠ¡ - ROSæœåŠ¡ç‰ˆæœ¬
VLA Language Service - ROS Service Version

é•¿æ—¶é—´å¯¹è¯æ§åˆ¶æµç¨‹æœåŠ¡
Long-term Dialogue Control Service

å®æ—¶è¯­éŸ³å½•åˆ¶ -> LLMåˆ†æ -> JSONç”Ÿæˆ -> TTSåé¦ˆ
"""

import os
import sys
import time
import asyncio
import logging
import json
import numpy as np
import threading
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import queue
import signal
import wave
import select
import termios
import tty

# ROSç›¸å…³å¯¼å…¥
import rclpy
from rclpy.node import Node
from rclpy.qos import QoSProfile, QoSDurabilityPolicy, QoSHistoryPolicy, QoSReliabilityPolicy
from std_msgs.msg import String
from std_srvs.srv import Trigger
from vla_language.msg import VLAIntent, VLAAction, VLACommand
from vla_language.srv import ProcessText, GetIntent, GenerateAction

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥å¿…è¦çš„åŒ…
try:
    import whisper
    WHISPER_AVAILABLE = True
    logger.info("âœ… Whisperå·²åŠ è½½")
except ImportError:
    WHISPER_AVAILABLE = False
    logger.error("âŒ Whisperæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openai-whisper")

try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
    logger.info("âœ… Edge TTSå·²åŠ è½½")
except ImportError:
    EDGE_TTS_AVAILABLE = False
    logger.error("âŒ Edge TTSæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install edge-tts")

try:
    import sounddevice as sd
    SOUND_DEVICE_AVAILABLE = True
    logger.info("âœ… SoundDeviceå·²åŠ è½½")
except ImportError:
    SOUND_DEVICE_AVAILABLE = False
    logger.error("âŒ SoundDeviceæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install sounddevice")

try:
    import soundfile as sf
    SOUND_FILE_AVAILABLE = True
    logger.info("âœ… SoundFileå·²åŠ è½½")
except ImportError:
    SOUND_FILE_AVAILABLE = False
    logger.error("âŒ SoundFileæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install soundfile")

try:
    import pygame
    PYGAME_AVAILABLE = True
    logger.info("âœ… PyGameå·²åŠ è½½")
except ImportError:
    PYGAME_AVAILABLE = False
    logger.error("âŒ PyGameæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install pygame")

# æ•°æ®ç±»å®šä¹‰
@dataclass
class RecognitionResult:
    """è¯­éŸ³è¯†åˆ«ç»“æœ"""
    text: str
    confidence: float
    duration: float
    engine: str = "whisper"

@dataclass
class IntentResult:
    """æ„å›¾è¯†åˆ«ç»“æœ"""
    intent: str
    confidence: float
    action: Dict[str, Any]
    processing_time: float

@dataclass
class TTSResult:
    """TTSç”Ÿæˆç»“æœ"""
    audio_path: str
    duration: float
    text: str
    engine: str = "edge_tts"

class AudioRecorder:
    """éŸ³é¢‘å½•åˆ¶å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.sample_rate = config.get('sample_rate', 16000)
        self.channels = config.get('channels', 1)
        self.chunk_duration = config.get('chunk_duration', 0.5)
        self.chunk_size = int(self.sample_rate * self.chunk_duration)
        self.silence_threshold = config.get('silence_threshold', 0.01)
        self.silence_duration = config.get('silence_duration', 2.0)
        self.min_recording_duration = config.get('min_recording_duration', 1.0)
        
        self.is_recording = False
        self.audio_queue = queue.Queue()
        self.silence_counter = 0
        self.recording_start_time = 0
        self.last_sound_time = 0
        
        # éŸ³é¢‘ç¼“å†²åŒº
        self.audio_buffer = []
        self.silence_buffer = []
        
        logger.info(f"éŸ³é¢‘å½•åˆ¶å™¨åˆå§‹åŒ–: {self.sample_rate}Hz, {self.channels}ch")
    
    def _detect_silence(self, audio_data: np.ndarray) -> bool:
        """æ£€æµ‹é™éŸ³"""
        if len(audio_data) == 0:
            return True
        
        # è®¡ç®—éŸ³é¢‘èƒ½é‡
        energy = np.mean(np.abs(audio_data))
        return energy < self.silence_threshold
    
    def _audio_callback(self, indata, frames, time_info, status):
        """éŸ³é¢‘å›è°ƒå‡½æ•°"""
        if status:
            logger.warning(f"éŸ³é¢‘å›è°ƒçŠ¶æ€: {status}")
        
        # å°†éŸ³é¢‘æ•°æ®æ”¾å…¥é˜Ÿåˆ—
        self.audio_queue.put(indata.copy())
        
        # æ£€æµ‹é™éŸ³
        if self._detect_silence(indata):
            self.silence_counter += 1
            self.silence_buffer.append(indata.copy())
        else:
            # æœ‰å£°éŸ³ï¼Œé‡ç½®é™éŸ³è®¡æ•°å™¨
            if self.silence_counter > 0:
                self.audio_buffer.extend(self.silence_buffer)
                self.silence_buffer = []
            self.silence_counter = 0
            self.audio_buffer.append(indata.copy())
            self.last_sound_time = time.time()
    
    def start_recording(self):
        """å¼€å§‹å½•åˆ¶"""
        if self.is_recording:
            logger.warning("å·²ç»åœ¨å½•åˆ¶ä¸­")
            return
        
        self.is_recording = True
        self.audio_buffer = []
        self.silence_buffer = []
        self.silence_counter = 0
        self.recording_start_time = time.time()
        self.last_sound_time = time.time()
        
        logger.info("ğŸ¤ å¼€å§‹å½•åˆ¶éŸ³é¢‘...")
        
        # å¯åŠ¨éŸ³é¢‘æµ
        try:
            self.stream = sd.InputStream(
                samplerate=self.sample_rate,
                channels=self.channels,
                callback=self._audio_callback,
                blocksize=self.chunk_size,
                dtype=np.float32
            )
            self.stream.start()
        except Exception as e:
            logger.error(f"å¯åŠ¨éŸ³é¢‘æµå¤±è´¥: {str(e)}")
            self.is_recording = False
    
    def stop_recording(self) -> Optional[np.ndarray]:
        """åœæ­¢å½•åˆ¶å¹¶è¿”å›éŸ³é¢‘æ•°æ®"""
        if not self.is_recording:
            logger.warning("æ²¡æœ‰åœ¨å½•åˆ¶ä¸­")
            return None
        
        logger.info("ğŸ›‘ åœæ­¢å½•åˆ¶éŸ³é¢‘")
        
        # åœæ­¢éŸ³é¢‘æµ
        try:
            self.stream.stop()
            self.stream.close()
        except Exception as e:
            logger.error(f"åœæ­¢éŸ³é¢‘æµå¤±è´¥: {str(e)}")
        
        self.is_recording = False
        
        # ç­‰å¾…å‰©ä½™éŸ³é¢‘æ•°æ®å¤„ç†
        time.sleep(0.1)
        
        # åˆå¹¶æ‰€æœ‰éŸ³é¢‘æ•°æ®
        if self.audio_buffer:
            full_audio = np.concatenate(self.audio_buffer, axis=0)
            recording_duration = time.time() - self.recording_start_time
            logger.info(f"å½•åˆ¶å®Œæˆï¼Œæ—¶é•¿: {recording_duration:.2f}ç§’ï¼ŒéŸ³é¢‘é•¿åº¦: {len(full_audio)}")
            return full_audio
        
        return None
    
    def should_stop_recording(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢å½•åˆ¶"""
        if not self.is_recording:
            return False
        
        # æ£€æŸ¥æœ€å°å½•åˆ¶æ—¶é—´
        current_duration = time.time() - self.recording_start_time
        if current_duration < self.min_recording_duration:
            return False
        
        # æ£€æŸ¥é™éŸ³æŒç»­æ—¶é—´
        silence_duration = self.silence_counter * self.chunk_duration
        if silence_duration >= self.silence_duration:
            logger.info(f"æ£€æµ‹åˆ°é™éŸ³ {silence_duration:.1f}ç§’ï¼Œåœæ­¢å½•åˆ¶")
            return True
        
        return False
    
    def get_recording_status(self) -> Dict[str, Any]:
        """è·å–å½•åˆ¶çŠ¶æ€"""
        return {
            'is_recording': self.is_recording,
            'duration': time.time() - self.recording_start_time if self.is_recording else 0,
            'silence_counter': self.silence_counter,
            'buffer_size': len(self.audio_buffer)
        }

class ProductionSpeechRecognizer:
    """æ­£å¼ç‰ˆè¯­éŸ³è¯†åˆ«å™¨ - ä½¿ç”¨Whisper"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.model = None
        self.sample_rate = config.get('sample_rate', 16000)
        self.model_name = config.get('model_name', 'base')
        
        if WHISPER_AVAILABLE:
            self._load_model()
    
    def _load_model(self):
        """åŠ è½½Whisperæ¨¡å‹"""
        try:
            logger.info(f"åŠ è½½Whisperæ¨¡å‹: {self.model_name}")
            self.model = whisper.load_model(self.model_name)
            logger.info("âœ… Whisperæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.model = None
    
    def recognize_audio(self, audio_data: np.ndarray) -> Optional[RecognitionResult]:
        """è¯†åˆ«éŸ³é¢‘"""
        if not WHISPER_AVAILABLE or self.model is None:
            logger.error("Whisperä¸å¯ç”¨")
            return None
        
        try:
            start_time = time.time()
            
            # ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
            temp_path = "/tmp/temp_recording.wav"
            sf.write(temp_path, audio_data, self.sample_rate)
            
            logger.info("å¼€å§‹è¯†åˆ«éŸ³é¢‘...")
            result = self.model.transcribe(
                temp_path,
                language='zh',
                fp16=False,
                verbose=False
            )
            
            recognized_text = result['text'].strip()
            duration = time.time() - start_time
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = min(1.0, len(recognized_text) / 20 * (1.0 / max(duration, 0.1)))
            
            logger.info(f"è¯†åˆ«ç»“æœ: '{recognized_text}'")
            logger.info(f"è¯†åˆ«è€—æ—¶: {duration:.2f}ç§’")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_path):
                os.remove(temp_path)
            
            return RecognitionResult(
                text=recognized_text,
                confidence=confidence,
                duration=duration,
                engine="whisper"
            )
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘è¯†åˆ«å¤±è´¥: {str(e)}")
            return None

class ProductionTTSGenerator:
    """æ­£å¼ç‰ˆTTSç”Ÿæˆå™¨ - ä½¿ç”¨Edge TTS"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.voice = config.get('voice', 'zh-CN-XiaoxiaoNeural')
        self.rate = config.get('rate', '+0%')
        self.volume = config.get('volume', '+0%')
        
        if not EDGE_TTS_AVAILABLE:
            logger.error("Edge TTSä¸å¯ç”¨")
    
    async def generate_audio(self, text: str, output_path: str) -> Optional[TTSResult]:
        """ç”ŸæˆéŸ³é¢‘"""
        if not EDGE_TTS_AVAILABLE:
            logger.error("Edge TTSä¸å¯ç”¨")
            return None
        
        try:
            start_time = time.time()
            
            logger.info(f"å¼€å§‹ç”ŸæˆTTSéŸ³é¢‘: '{text[:50]}...'")
            
            # åˆ›å»ºEdge TTSé€šä¿¡å¯¹è±¡
            communicate = edge_tts.Communicate(
                text=text,
                voice=self.voice,
                rate=self.rate,
                volume=self.volume
            )
            
            # ç”ŸæˆéŸ³é¢‘
            await communicate.save(output_path)
            
            duration = time.time() - start_time
            
            # éªŒè¯æ–‡ä»¶
            if os.path.exists(output_path):
                file_size = os.path.getsize(output_path)
                logger.info(f"âœ… TTSç”ŸæˆæˆåŠŸ: {output_path} ({file_size} bytes)")
                logger.info(f"ç”Ÿæˆè€—æ—¶: {duration:.2f}ç§’")
                
                return TTSResult(
                    audio_path=output_path,
                    duration=duration,
                    text=text,
                    engine="edge_tts"
                )
            else:
                raise Exception("éŸ³é¢‘æ–‡ä»¶ç”Ÿæˆå¤±è´¥")
                
        except Exception as e:
            logger.error(f"TTSç”Ÿæˆå¤±è´¥: {str(e)}")
            return None
    
    def generate_audio_sync(self, text: str, output_path: str) -> Optional[TTSResult]:
        """åŒæ­¥ç”ŸæˆéŸ³é¢‘"""
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(self.generate_audio(text, output_path))
            loop.close()
            return result
        except Exception as e:
            logger.error(f"åŒæ­¥TTSç”Ÿæˆå¤±è´¥: {str(e)}")
            return None

class ProductionLLMProcessor:
    """æ­£å¼ç‰ˆLLMå¤„ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.engine_type = config.get('engine_type', 'zhipuai')
        self.max_tokens = config.get('max_tokens', 100)
        self.temperature = config.get('temperature', 0.1)
        self.timeout = config.get('timeout', 5.0)
        
        # æ™ºèƒ½ç¼“å­˜
        self.cache = {}
        self.cache_timeout = config.get('cache_timeout', 300)  # 5åˆ†é’Ÿ
        
        logger.info(f"åˆå§‹åŒ–LLMå¤„ç†å™¨: {self.engine_type}")
    
    def _get_cache_key(self, text: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def _is_cache_valid(self, cache_entry: Dict) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        return (time.time() - cache_entry['timestamp']) < self.cache_timeout
    
    async def process_text_async(self, text: str) -> IntentResult:
        """å¼‚æ­¥å¤„ç†æ–‡æœ¬"""
        start_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(text)
        if cache_key in self.cache:
            cache_entry = self.cache[cache_key]
            if self._is_cache_valid(cache_entry):
                logger.info(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: '{text[:30]}...'")
                self.cache_hits = getattr(self, 'cache_hits', 0) + 1
                return IntentResult(
                    intent=cache_entry['intent'],
                    confidence=cache_entry['confidence'],
                    action=cache_entry['action'],
                    processing_time=time.time() - start_time
                )
        
        try:
            # æ ¹æ®æ–‡æœ¬å†…å®¹ç¡®å®šæ„å›¾
            intent, confidence, action = self._classify_intent(text)
            
            # æ›´æ–°ç¼“å­˜
            self.cache[cache_key] = {
                'intent': intent,
                'confidence': confidence,
                'action': action,
                'timestamp': time.time()
            }
            
            processing_time = time.time() - start_time
            
            logger.info(f"LLMå¤„ç†ç»“æœ: {intent}, ç½®ä¿¡åº¦: {confidence:.2f}, è€—æ—¶: {processing_time:.2f}ç§’")
            
            return IntentResult(
                intent=intent,
                confidence=confidence,
                action=action,
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"LLMå¤„ç†å¤±è´¥: {str(e)}")
            return IntentResult(
                intent="unknown",
                confidence=0.0,
                action={},
                processing_time=time.time() - start_time
            )
    
    def _classify_intent(self, text: str) -> Tuple[str, float, Dict[str, Any]]:
        """åˆ†ç±»æ„å›¾ - ä½¿ç”¨JSONé…ç½®æ–‡ä»¶"""
        text = text.lower().strip()
        
        # åŠ è½½æ„å›¾é…ç½®
        intent_config_path = os.path.join(os.path.dirname(__file__), '..', 'config', 'intent_patterns.json')
        intent_config = {}
        
        try:
            if os.path.exists(intent_config_path):
                with open(intent_config_path, 'r', encoding='utf-8') as f:
                    intent_config = json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½æ„å›¾é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        best_intent = "unknown"
        best_confidence = 0.0
        best_action = {}
        
        # éå†æ‰€æœ‰æ„å›¾
        for intent_key, intent_data in intent_config.get('intents', {}).items():
            patterns = intent_data.get('patterns', [])
            confidence_threshold = intent_data.get('confidence_threshold', 0.0)
            
            # è®¡ç®—åŒ¹é…åº¦
            match_count = sum(1 for pattern in patterns if pattern in text)
            confidence = match_count / len(patterns) if patterns else 0.0
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
            if confidence > best_confidence and confidence >= confidence_threshold:
                best_confidence = confidence
                best_intent = intent_key
                
                # è·å–æ„å›¾é…ç½®
                response_text = intent_data.get('response', '')
                action_config = intent_data.get('action', {})
                json_response_config = intent_data.get('json_response', {})
                
                # æ›´æ–°JSONå“åº”ä¸­çš„æ—¶é—´æˆ³
                json_response_config['timestamp'] = time.time()
                json_response_config['confidence'] = confidence
                
                # æ„å»ºåŠ¨ä½œ
                best_action = {
                    'type': action_config.get('type', 'response'),
                    'text': response_text,
                    'json_response': json_response_config
                }
                
                # æ·»åŠ å…¶ä»–åŠ¨ä½œå‚æ•°
                for key, value in action_config.items():
                    if key != 'type':
                        best_action[key] = value
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æ„å›¾ï¼Œä½¿ç”¨é»˜è®¤çš„unknownæ„å›¾
        if best_confidence == 0.0:
            unknown_intent = intent_config.get('intents', {}).get('unknown', {})
            best_action = {
                'type': 'response',
                'text': unknown_intent.get('response', 'æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„è¯·æ±‚ã€‚è¯·å°è¯•ç”¨æ›´æ¸…æ™°çš„è¯­è¨€è¡¨è¾¾ã€‚'),
                'json_response': unknown_intent.get('json_response', {})
            }
            best_action['json_response']['timestamp'] = time.time()
            best_action['json_response']['confidence'] = 0.0
        
        return best_intent, best_confidence, best_action

class VLALanguageService(Node):
    """VLAè¯­è¨€æœåŠ¡èŠ‚ç‚¹"""
    
    def __init__(self):
        super().__init__('vla_language_service')
        
        # é…ç½®å‚æ•°
        self.declare_parameter('audio.sample_rate', 16000)
        self.declare_parameter('audio.channels', 1)
        self.declare_parameter('audio.chunk_duration', 0.5)
        self.declare_parameter('audio.silence_threshold', 0.01)
        self.declare_parameter('audio.silence_duration', 2.0)
        self.declare_parameter('audio.min_recording_duration', 1.0)
        self.declare_parameter('speech.model_name', 'base')
        self.declare_parameter('tts.voice', 'zh-CN-XiaoxiaoNeural')
        self.declare_parameter('tts.rate', '+0%')
        self.declare_parameter('tts.volume', '+0%')
        self.declare_parameter('llm.engine_type', 'zhipuai')
        self.declare_parameter('llm.max_tokens', 100)
        self.declare_parameter('llm.temperature', 0.1)
        self.declare_parameter('llm.timeout', 5.0)
        self.declare_parameter('llm.cache_timeout', 300)
        
        # è·å–é…ç½®
        config = self._get_config()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.recorder = AudioRecorder(config.get('audio', {}))
        self.speech_recognizer = ProductionSpeechRecognizer(config.get('speech', {}))
        self.tts_generator = ProductionTTSGenerator(config.get('tts', {}))
        self.llm_processor = ProductionLLMProcessor(config.get('llm', {}))
        
        # åˆ›å»ºéŸ³é¢‘ç›®å½•
        self.audio_dir = os.path.join(os.path.dirname(__file__), 'audio')
        if not os.path.exists(self.audio_dir):
            os.makedirs(self.audio_dir)
        
        # åˆ›å»ºæœåŠ¡
        self.create_service(ProcessText, self.process_text_callback)
        self.create_service(GetIntent, self.get_intent_callback)
        self.create_service(GenerateAction, self.generate_action_callback)
        self.create_service(Trigger, self.start_recording_callback)
        self.create_service(Trigger, self.stop_recording_callback)
        
        # åˆ›å»ºå‘å¸ƒè€…
        self.intent_publisher = self.create_publisher(VLAIntent, 'vla_intent', 10)
        self.action_publisher = self.create_publisher(VLAAction, 'vla_action', 10)
        self.command_publisher = self.create_publisher(VLACommand, 'vla_command', 10)
        self.latest_intent_publisher = self.create_publisher(String, '/vla_language/latest_intent', 10)
        
        # åˆ›å»ºè®¡æ—¶å™¨
        self.timer = self.create_timer(0.1, self.timer_callback)
        
        # ç³»ç»ŸçŠ¶æ€
        self.is_recording = False
        self.current_intent = None
        self.current_action = None
        
        logger.info("âœ… VLAè¯­è¨€æœåŠ¡å·²å¯åŠ¨")
    
    def _get_config(self) -> Dict[str, Any]:
        """è·å–é…ç½®"""
        return {
            'audio': {
                'sample_rate': self.get_parameter('audio.sample_rate').value,
                'channels': self.get_parameter('audio.channels').value,
                'chunk_duration': self.get_parameter('audio.chunk_duration').value,
                'silence_threshold': self.get_parameter('audio.silence_threshold').value,
                'silence_duration': self.get_parameter('audio.silence_duration').value,
                'min_recording_duration': self.get_parameter('audio.min_recording_duration').value
            },
            'speech': {
                'model_name': self.get_parameter('speech.model_name').value
            },
            'tts': {
                'voice': self.get_parameter('tts.voice').value,
                'rate': self.get_parameter('tts.rate').value,
                'volume': self.get_parameter('tts.volume').value
            },
            'llm': {
                'engine_type': self.get_parameter('llm.engine_type').value,
                'max_tokens': self.get_parameter('llm.max_tokens').value,
                'temperature': self.get_parameter('llm.temperature').value,
                'timeout': self.get_parameter('llm.timeout').value,
                'cache_timeout': self.get_parameter('llm.cache_timeout').value
            }
        }
    
    def process_text_callback(self, request, response):
        """å¤„ç†æ–‡æœ¬æœåŠ¡å›è°ƒ"""
        try:
            start_time = time.time()
            
            # å¤„ç†æ–‡æœ¬
            intent_result = asyncio.run(self.llm_processor.process_text_async(request.text))
            
            # åˆ›å»ºå“åº”
            response.success = True
            response.intent = intent_result.intent
            response.confidence = intent_result.confidence
            response.processing_time = intent_result.processing_time
            
            # ç”ŸæˆJSONå“åº”
            json_response = intent_result.action.get('json_response', {})
            json_response['timestamp'] = time.time()
            json_response['confidence'] = intent_result.confidence
            json_response['original_text'] = request.text
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            json_path = os.path.join(self.audio_dir, "latest_response.json")
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(json_response, f, ensure_ascii=False, indent=2)
            
            # å‘å¸ƒæ„å›¾æ¶ˆæ¯
            intent_msg = VLAIntent()
            intent_msg.intent = intent_result.intent
            intent_msg.confidence = intent_result.confidence
            intent_msg.timestamp = time.time()
            self.intent_publisher.publish(intent_msg)
            
            # å‘å¸ƒåŠ¨ä½œæ¶ˆæ¯
            if intent_result.action:
                action_msg = VLAAction()
                action_msg.type = intent_result.action.get('type', 'response')
                action_msg.text = intent_result.action.get('text', '')
                action_msg.json_data = json.dumps(intent_result.action.get('json_response', {}))
                self.action_publisher.publish(action_msg)
            
            # å‘å¸ƒæœ€æ–°çš„æ„å›¾æ•°æ®åˆ°è¯é¢˜
            latest_intent_msg = String()
            latest_intent_data = {
                'intent': intent_result.intent,
                'confidence': intent_result.confidence,
                'response': json_response.get('response', ''),
                'instruction': json_response.get('instruction', 'none'),
                'original_text': request.text,
                'timestamp': time.time()
            }
            latest_intent_msg.data = json.dumps(latest_intent_data, ensure_ascii=False)
            self.latest_intent_publisher.publish(latest_intent_msg)
            
            logger.info(f"æ–‡æœ¬å¤„ç†å®Œæˆ: {intent_result.intent} (ç½®ä¿¡åº¦: {intent_result.confidence:.2f})")
            
        except Exception as e:
            self.get_logger().error(f"æ–‡æœ¬å¤„ç†å¤±è´¥: {str(e)}")
            response.success = False
            response.message = str(e)
        
        return response
    
    def get_intent_callback(self, request, response):
        """è·å–æ„å›¾æœåŠ¡å›è°ƒ"""
        try:
            # ä»æ–‡ä»¶è¯»å–æœ€æ–°çš„æ„å›¾
            json_path = os.path.join(self.audio_dir, "latest_response.json")
            if os.path.exists(json_path):
                with open(json_path, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)
                
                response.success = True
                response.intent = json_data.get('intent', 'unknown')
                response.confidence = json_data.get('confidence', 0.0)
                response.timestamp = json_data.get('timestamp', 0.0)
            else:
                response.success = False
                response.message = "æ²¡æœ‰æ‰¾åˆ°æ„å›¾æ•°æ®"
                
        except Exception as e:
            self.get_logger().error(f"è·å–æ„å›¾å¤±è´¥: {str(e)}")
            response.success = False
            response.message = str(e)
        
        return response
    
    def generate_action_callback(self, request, response):
        """ç”ŸæˆåŠ¨ä½œæœåŠ¡å›è°ƒ"""
        try:
            # å¤„ç†æ–‡æœ¬ç”ŸæˆåŠ¨ä½œ
            intent_result = asyncio.run(self.llm_processor.process_text_async(request.text))
            
            response.success = True
            response.action_type = intent_result.action.get('type', 'response')
            response.action_text = intent_result.action.get('text', '')
            response.json_data = json.dumps(intent_result.action.get('json_response', {}))
            
            # å‘å¸ƒå‘½ä»¤æ¶ˆæ¯
            command_msg = VLACommand()
            command_msg.type = intent_result.action.get('type', 'response')
            command_msg.text = intent_result.action.get('text', '')
            command_msg.json_data = json.dumps(intent_result.action.get('json_response', {}))
            self.command_publisher.publish(command_msg)
            
            logger.info(f"åŠ¨ä½œç”Ÿæˆå®Œæˆ: {intent_result.action.get('type', 'response')}")
            
        except Exception as e:
            self.get_logger().error(f"ç”ŸæˆåŠ¨ä½œå¤±è´¥: {str(e)}")
            response.success = False
            response.message = str(e)
        
        return response
    
    def start_recording_callback(self, request, response):
        """å¼€å§‹å½•åˆ¶æœåŠ¡å›è°ƒ"""
        try:
            if not self.is_recording:
                self.recorder.start_recording()
                self.is_recording = True
                response.success = True
                response.message = "å¼€å§‹å½•åˆ¶éŸ³é¢‘"
                logger.info("ğŸ¤ å¼€å§‹å½•åˆ¶éŸ³é¢‘")
            else:
                response.success = False
                response.message = "å·²ç»åœ¨å½•åˆ¶ä¸­"
                
        except Exception as e:
            self.get_logger().error(f"å¼€å§‹å½•åˆ¶å¤±è´¥: {str(e)}")
            response.success = False
            response.message = str(e)
        
        return response
    
    def stop_recording_callback(self, request, response):
        """åœæ­¢å½•åˆ¶æœåŠ¡å›è°ƒ"""
        try:
            if self.is_recording:
                audio_data = self.recorder.stop_recording()
                self.is_recording = False
                
                if audio_data is not None:
                    # å¤„ç†éŸ³é¢‘
                    self._process_audio(audio_data)
                    response.success = True
                    response.message = "å½•åˆ¶å®Œæˆå¹¶å¤„ç†"
                else:
                    response.success = False
                    response.message = "å½•åˆ¶å¤±è´¥"
            else:
                response.success = False
                response.message = "æ²¡æœ‰åœ¨å½•åˆ¶ä¸­"
                
        except Exception as e:
            self.get_logger().error(f"åœæ­¢å½•åˆ¶å¤±è´¥: {str(e)}")
            response.success = False
            response.message = str(e)
        
        return response
    
    def _process_audio(self, audio_data: np.ndarray):
        """å¤„ç†éŸ³é¢‘"""
        try:
            # 1. è¯­éŸ³è¯†åˆ«
            recognition_result = self.speech_recognizer.recognize_audio(audio_data)
            if not recognition_result:
                logger.error("è¯­éŸ³è¯†åˆ«å¤±è´¥")
                return
            
            # 2. LLMå¤„ç†
            llm_result = asyncio.run(self.llm_processor.process_text_async(recognition_result.text))
            
            # 3. ç”ŸæˆJSONå“åº”
            json_response = llm_result.action.get('json_response', {})
            json_response['recognition'] = {
                'text': recognition_result.text,
                'confidence': recognition_result.confidence,
                'duration': recognition_result.duration,
                'engine': recognition_result.engine
            }
            json_response['original_text'] = recognition_result.text
            
            # åªä¿å­˜ä¸€ä¸ªæœ€æ–°çš„JSONæ–‡ä»¶
            json_path = os.path.join(self.audio_dir, "latest_response.json")
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(json_response, f, ensure_ascii=False, indent=2)
            
            logger.info(f"JSONå“åº”å·²ä¿å­˜: {json_path}")
            
            # å‘å¸ƒæ„å›¾æ¶ˆæ¯
            intent_msg = VLAIntent()
            intent_msg.intent = llm_result.intent
            intent_msg.confidence = llm_result.confidence
            intent_msg.timestamp = time.time()
            self.intent_publisher.publish(intent_msg)
            
            # å‘å¸ƒåŠ¨ä½œæ¶ˆæ¯
            if llm_result.action:
                action_msg = VLAAction()
                action_msg.type = llm_result.action.get('type', 'response')
                action_msg.text = llm_result.action.get('text', '')
                action_msg.json_data = json.dumps(llm_result.action.get('json_response', {}))
                self.action_publisher.publish(action_msg)
            
            # å‘å¸ƒæœ€æ–°çš„æ„å›¾æ•°æ®åˆ°è¯é¢˜
            latest_intent_msg = String()
            latest_intent_data = {
                'intent': llm_result.intent,
                'confidence': llm_result.confidence,
                'response': json_response.get('response', ''),
                'instruction': json_response.get('instruction', 'none'),
                'original_text': recognition_result.text,
                'timestamp': time.time()
            }
            latest_intent_msg.data = json.dumps(latest_intent_data, ensure_ascii=False)
            self.latest_intent_publisher.publish(latest_intent_msg)
            
            # 4. ç”ŸæˆTTSå“åº”
            if llm_result.action.get('type') == 'response':
                tts_output_path = os.path.join(self.audio_dir, f"tts_{int(time.time())}.wav")
                tts_result = self.tts_generator.generate_audio_sync(
                    llm_result.action['text'], 
                    tts_output_path
                )
                
                if tts_result:
                    logger.info(f"TTSéŸ³é¢‘å·²ç”Ÿæˆ: {tts_output_path}")
                    # æ’­æ”¾éŸ³é¢‘
                    self._play_audio(tts_output_path)
            
            logger.info(f"å¤„ç†å®Œæˆ: {llm_result.intent} (ç½®ä¿¡åº¦: {llm_result.confidence:.2f})")
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")
    
    def _play_audio(self, audio_path: str):
        """æ’­æ”¾éŸ³é¢‘"""
        try:
            if not PYGAME_AVAILABLE:
                logger.warning("PyGameæœªå®‰è£…ï¼Œæ— æ³•æ’­æ”¾éŸ³é¢‘")
                return
            
            pygame.mixer.init()
            pygame.mixer.music.load(audio_path)
            pygame.mixer.music.play()
            while pygame.mixer.music.get_busy():
                pygame.time.Clock().tick(10)
            pygame.mixer.music.stop()
            pygame.mixer.quit()
        except Exception as e:
            logger.error(f"æ’­æ”¾éŸ³é¢‘å¤±è´¥: {str(e)}")
    
    def timer_callback(self):
        """å®šæ—¶å™¨å›è°ƒ"""
        if self.is_recording:
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢å½•åˆ¶
            if self.recorder.should_stop_recording():
                audio_data = self.recorder.stop_recording()
                self.is_recording = False
                
                if audio_data is not None:
                    # å¤„ç†éŸ³é¢‘
                    self._process_audio(audio_data)

def main(args=None):
    """ä¸»å‡½æ•°"""
    rclpy.init(args=args)
    
    # æ£€æŸ¥ä¾èµ–
    if not WHISPER_AVAILABLE:
        print("âŒ Whisperä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install openai-whisper")
        return
    
    if not EDGE_TTS_AVAILABLE:
        print("âŒ Edge TTSä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install edge-tts")
        return
    
    if not SOUND_DEVICE_AVAILABLE:
        print("âŒ SoundDeviceä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install sounddevice")
        return
    
    if not SOUND_FILE_AVAILABLE:
        print("âŒ SoundFileä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install soundfile")
        return
    
    # åˆ›å»ºæœåŠ¡èŠ‚ç‚¹
    service_node = VLALanguageService()
    
    try:
        rclpy.spin(service_node)
    except KeyboardInterrupt:
        print("ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨å…³é—­æœåŠ¡...")
    finally:
        service_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
